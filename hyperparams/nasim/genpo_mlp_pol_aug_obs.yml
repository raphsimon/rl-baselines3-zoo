GenPO-v0:
  env_wrapper: nasim.generalized_envs.augmented_obs_wrapper.AugmentedObsWrapper
  n_envs: 16
  n_timesteps: !!float 1e7
  policy: 'MlpPolicy'
  batch_size: 128
  clip_range: 0.4
  ent_coef: 0.05
  gae_lambda: 0.95
  gamma: 0.995
  learning_rate: 0.00003
  max_grad_norm: 2
  n_epochs: 20
  n_steps: 128
  vf_coef: 0.3
  policy_kwargs: "dict(                                                                                                                         
                    ortho_init=False,
                    activation_fn=nn.Tanh,
		    net_arch=dict(pi=[256], vf=[256])
                )"
